{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 垃圾邮件分类\n",
    "\n",
    "- 如何去开展一个项目\n",
    "\n",
    "  熟悉数据\n",
    "  清洗数据\n",
    "  数据分析\n",
    "  选择模型\n",
    "  建立模型\n",
    "  \n",
    "  训练模型\n",
    "  模型评估\n",
    "  模型优化（调参）\n",
    "  \n",
    "  落地上线\n",
    "  \n",
    "- 项目的制作流程\n",
    "- 如何把手写的算法实现到真实的项目中去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 朴素贝叶斯分类器训练函数\n",
    "# 训练数据\n",
    "\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    '''\n",
    "    Desc:\n",
    "        分类器训练函数\n",
    "    Args:\n",
    "        trainMatrix:训练数据[0,1,0,1,0,2,0]\n",
    "        trainCategory:训练类别 eg:[1,0,1,0]\n",
    "    return:\n",
    "        \n",
    "    '''\n",
    "    # 总文件数\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    # 总单词数\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # 侮辱性文件的出现概率 = 侮辱性文件数/总文件数\n",
    "    # [1，0，1，0，1，0，0]\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    \n",
    "    # 构造单词出现次数列表\n",
    "    # p0Num 正常的统计\n",
    "    # P1Num 侮辱的统计\n",
    "    # [1,2,1]\n",
    "    p0Num = np.ones(numWords)\n",
    "    p1Num = np.ones(numWords)\n",
    "\n",
    "    # 整个数据集单词出现的总数，根据样本/实际调查结果调整分母的值\n",
    "    global p0Denom\n",
    "    global p1Denom\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "    # 遍历总文件数\n",
    "\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1: # 垃圾邮件\n",
    "            # 累加辱骂词的频次\n",
    "            p1Num += trainMatrix[i]\n",
    "            # 对每篇文章的辱骂的频次 进行统计汇总\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:  # 非垃圾邮件\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # P(单词｜类别) = p(单词1｜类别1)*p(单词2｜类别1)\n",
    "    # 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表\n",
    "    p1Vect = np.log(p1Num / p1Denom)\n",
    "    # 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表\n",
    "    p0Vect = np.log(p0Num / p0Denom)\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切分文本\n",
    "def textParse(bigString):\n",
    "    '''\n",
    "    Desc:\n",
    "        接收一个大字符串并将其解析为字符串列表\n",
    "    Args:\n",
    "        bigString -- 大字符串\n",
    "        jieba\n",
    "    Returns:\n",
    "        去掉少于 2 个字符的字符串，并将所有字符串转换为小写，返回字符串列表\n",
    "    '''\n",
    "    import re\n",
    "    # 使用正则表达式来切分句子，其中分隔符是除单词、数字外的任意字符串\n",
    "    listOfTokens = re.split(r'\\W+', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建所有单词的集合\n",
    "def createVocabList(dataSet):\n",
    "    \"\"\"\n",
    "    获取所有单词的集合\n",
    "    :param dataSet: 数据集\n",
    "    :return: 所有单词的集合(即不含重复元素的单词列表)\n",
    "    \"\"\"\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        # 操作符 | 用于求两个集合的并集\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)\n",
    "\n",
    "# 词向量转化\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    \"\"\"\n",
    "    遍历查看该单词是否出现，出现该单词则将该单词置1\n",
    "    :param vocabList: 所有单词集合列表\n",
    "    :param inputSet: 输入数据集\n",
    "    :return: 匹配列表[0,1,0,1...]，其中 1与0 表示词汇表中的单词是否出现在输入的数据集中\n",
    "    \"\"\"\n",
    "    # 创建一个和词汇表等长的向量，并将其元素都设置为0\n",
    "    returnVec = [0] * len(vocabList)  # [0,0......]\n",
    "    # 遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 朴素贝叶斯分类函数\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    # 单词表*每个单词在p0/p1下出现的概率+该类别的概率（做比较）\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spamTest():\n",
    "    '''\n",
    "    Desc:\n",
    "        对贝叶斯垃圾邮件分类器进行自动化处理。\n",
    "    Args:\n",
    "        none\n",
    "    Returns:\n",
    "        对测试集中的每封邮件进行分类，若邮件分类错误，则错误数加 1，最后返回总的错误百分比。\n",
    "    '''\n",
    "    # 单词表\n",
    "    docList = []\n",
    "    # 类别表\n",
    "    classList = []\n",
    "    # 文本表\n",
    "    fullText = []\n",
    "    # 把所有文本划分到相应的列表中\n",
    "    for i in range(1, 26):\n",
    "        # 切分，解析数据，并归类为 1 类别\n",
    "        wordList = textParse(\n",
    "            open(r'4.NaiveBayes/email/spam/%d.txt' % i,\n",
    "                 encoding='ISO-8859-1').read())\n",
    "        docList.append(wordList)\n",
    "        classList.append(1)\n",
    "        # 切分，解析数据，并归类为 0 类别\n",
    "        wordList = textParse(\n",
    "            open(r'4.NaiveBayes/email/ham/%d.txt' % i,\n",
    "                 encoding='ISO-8859-1').read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    # 创建所有单词，词汇表\n",
    "    vocabList = createVocabList(docList)\n",
    "    trainingSet = list(range(50))\n",
    "    # train = 50 - 10 =40\n",
    "    testSet = []\n",
    "    # 随机取 10 个邮件用来测试\n",
    "    for i in range(10):\n",
    "        # random.uniform(x, y) 随机生成一个范围为 x ~ y 的实数\n",
    "        randIndex = int(random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del (trainingSet[randIndex])\n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "    # 训练数据\n",
    "    # [0,1,0,1,0,0,1]\n",
    "    # [1,0,0,1,1,1,1]\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))\n",
    "    errorCount = 0\n",
    "    # 测试数据\n",
    "    for docIndex in testSet:\n",
    "        wordVector = setOfWords2Vec(vocabList, docList[docIndex])\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V,\n",
    "                      pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print('the errorCount is: ', errorCount)\n",
    "    print('the testSet length is :', len(testSet))\n",
    "    print('the error rate is :', float(errorCount) / len(testSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the errorCount is:  0\n",
      "the testSet length is : 10\n",
      "the error rate is : 0.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "spamTest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
